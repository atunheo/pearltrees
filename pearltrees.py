import streamlit as st
import pandas as pd
import requests
import re
import time
from io import BytesIO

st.set_page_config(page_title="heo Ãº", page_icon="ðŸŒ¿", layout="centered")

API_URL = "https://www.pearltrees.com/s/treeandpearlsapi/getPearlParentTreeAndSiblingPearls"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"
    ),
    "Accept": "application/json, text/plain, */*",
    "X-Requested-With": "XMLHttpRequest",
    "Origin": "https://www.pearltrees.com",
}

def extract_pearl_id(url: str):
    """TÃ¡ch pearlId tá»« URL kiá»ƒu .../item123456"""
    m = re.search(r"item(\d+)", url)
    if m:
        return int(m.group(1))
    m = re.search(r"pearlId=(\d+)", url)
    return int(m.group(1)) if m else None

def get_related_pearl_ids(pearl_id: int):
    """Láº¥y danh sÃ¡ch pearl con / anh em"""
    try:
        r = requests.get(API_URL, params={"pearlId": pearl_id}, headers=HEADERS, timeout=10)
        if r.status_code != 200:
            return []
        data = r.json()
        ids = set()
        def extract(obj):
            if isinstance(obj, dict):
                if "id" in obj and isinstance(obj["id"], int):
                    ids.add(obj["id"])
                for v in obj.values():
                    extract(v)
            elif isinstance(obj, list):
                for it in obj:
                    extract(it)
        extract(data)
        return list(ids)
    except Exception:
        return []

def crawl_tree(seed_id: int, limit=1000, delay=0.3):
    """Duyá»‡t toÃ n bá»™ cÃ¢y tá»« seed_id"""
    visited, to_visit = set(), [seed_id]
    results = []
    progress = st.progress(0)
    step = 0
    while to_visit and len(visited) < limit:
        current = to_visit.pop(0)
        if current in visited:
            continue
        visited.add(current)
        results.append(current)
        related = get_related_pearl_ids(current)
        for rid in related:
            if rid not in visited and rid not in to_visit:
                to_visit.append(rid)
        step += 1
        progress.progress(min(step / limit, 1.0))
        time.sleep(delay)
    return sorted(results)

def get_final_url(url):
    """Theo dÃµi redirect Ä‘á»ƒ láº¥y URL cuá»‘i cÃ¹ng"""
    try:
        r = requests.head(url, allow_redirects=True, timeout=10)
        return r.url
    except Exception:
        return url

# ---------- Streamlit UI ----------
st.title("ðŸŒ¿ heo con dá»… thÆ°Æ¡ng ")
st.markdown("""
hello world nÃ¨ 
""")

col1, col2 = st.columns(2)
with col1:
    username = st.text_input("ðŸ‘¤ TÃªn tÃ i khoáº£n (vd: heiliaounu):", "")
with col2:
    start_url = st.text_input("ðŸŒ link bÃ i viáº¿t :", "")

max_items = st.number_input("sá»‘ lÆ°á»£ng bÃ i viáº¿t  crawl", min_value=10, max_value=5000, value=500)
delay = st.slider("Äá»™ trá»… giá»¯a cÃ¡c request (giÃ¢y)", min_value=0.0, max_value=3.0, value=0.3, step=0.1)

if st.button("ðŸ–ðŸ–ðŸ– Báº¯t Ä‘áº§u Crawl"):
    if not username and not start_url:
        st.warning("âš ï¸ Cáº§n nháº­p username hoáº·c URL item.")
    else:
        seed_id = None
        if start_url:
            seed_id = extract_pearl_id(start_url)
        if not seed_id:
            st.error("âŒ KhÃ´ng tÃ¬m tháº¥y pearlId trong URL.")
        else:
            st.info(f"ðŸ” Seed pearlId = {seed_id}")
            with st.spinner("Äang crawl danh sÃ¡ch ID..."):
                pearl_ids = crawl_tree(seed_id, limit=max_items, delay=delay)
                st.success(f"âœ… Crawl xong {len(pearl_ids)} ID. Äang láº¥y Final Link...")

            progress = st.progress(0)
            final_links = []
            total = len(pearl_ids)

            for i, pid in enumerate(pearl_ids):
                raw_link = f"https://www.pearltrees.com/{username}/item{pid}"
                final_url = get_final_url(raw_link)
                final_links.append(final_url)
                progress.progress(min((i + 1) / total, 1.0))
                time.sleep(delay)

            df = pd.DataFrame(final_links, columns=["Final Link"])
            df = df.drop_duplicates().sort_values(by="Final Link").reset_index(drop=True)

            st.success(f"âœ… HoÃ n táº¥t! Thu Ä‘Æ°á»£c {len(df)} final links.")
            st.dataframe(df)

            buffer = BytesIO()
            df.to_excel(buffer, index=False, engine="openpyxl")
            buffer.seek(0)
            st.download_button(
                label="ðŸ“¥ Táº£i file Excel Final Link",
                data=buffer,
                file_name=f"{username}_final_links.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
